---
title: "Journal (reproducible report)"
author: "Lucas Ernst"
date: "2020-12-01"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE,error = TRUE)
```

# Intro to the tidyverse
```{r}
# Data Science at TUHH ------------------------------------------------------
# SALES ANALYSIS ----

# 1.0 Load libraries ----
library(tidyverse)
library(readxl)
library(lubridate)
library(ggplot2)


# 2.0 Importing Files ----
bikes <- read_excel("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
bikeshops <- read_excel("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
orderlines <- read_excel("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")

# 3.0 Examining Data ----
glimpse(bikes)
bikes %>% select(c(model,bike.id,model.year))

# 4.0 Joining Data ----

bike_orderlines_joined <- orderlines %>%
  left_join(bikes, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops, by = c("customer.id" = "bikeshop.id"))
glimpse(bike_orderlines_joined)
# 5.0 Wrangling Data ----
bike_orderlines_wrangled <- bike_orderlines_joined %>%
separate(category,into = c("category.1","category.2","category.3"
                           ,sep = " - ")) %>%
  mutate(total.price=price * quantity) %>%
  set_names(names(.) %>% str_replace_all("\\.", "_"))


bike_orderlines_groupedbycategory <- bike_orderlines_wrangled %>% 
  group_by(category_1) %>%
  summarize(total_price = sum(price*quantity)) 

# 6.0 Business Insights ----
# 6.1 Sales by Year ----

# Step 1 - Manipulate
bike_orderlines_salesbyyear <- bike_orderlines_wrangled %>% 
  transmute(date=year(order_date),total_price) %>%
  group_by(date) %>%
  summarise(total_sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(total_sales,big.mark = ".", 
                                   decimal.mark = ",", suffix = " €"))
# Step 2 - Visualize
bike_orderlines_salesbyyear %>% 
  ggplot(aes(x=date,y=total_sales)) + 
  geom_bar(stat = "identity") +
  geom_col(fill="#2DC6D6") +
  geom_label(aes(label = sales_text)) +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year",
    subtitle = "Upward Trend",
    x = "",
    y = "Revenue"
  )
# 6.2 Sales by Year and Category 2 ----

# Step 1 - Manipulate
bike_orderlines_salesbyyearcategory <-  bike_orderlines_wrangled %>% 
  transmute(date=year(order_date),total_price, category_1) %>%
  group_by(date,category_1) %>%
  summarise(total_sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(total_sales,big.mark = ".", 
                                     decimal.mark = ",", suffix = " €"))
# Step 2 - Visualize

bike_orderlines_salesbyyearcategory %>% 
  ggplot(aes(x=date,y=total_sales, fill = category_1)) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and category",
    subtitle = "Each product category has an upward trend",
    x = "",
    y = "Revenue",
    fill = "Main Category"
  ) +
  facet_wrap(vars(category_1))

# 7.0 Writing Files ----

# 7.1 Excel ----
library(writexl)
bike_orderlines_wrangled %>%
  write_xlsx("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.xlsx")
# 7.2 CSV ----
bike_orderlines_wrangled %>%
  write_csv("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.csv")
# 7.3 RDS ----
bike_orderlines_wrangled %>%
  write_rds("D:/Lucas/Uni/Data Science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds")
```

## Challenge
```{r}

# 8.0 Challenge ----

# Step 1 - Manipulate
bike_orderlines_wrangled_location <- bike_orderlines_wrangled %>%
  transmute(location, total_price, year = year(order_date)) %>%
  separate(col = location, into = c("city","state"),sep=", ") %>%
  group_by(state, year) %>%
  summarise(sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(sales,big.mark = ".", 
                                     decimal.mark = ",", suffix = " €"))
  
# Step 2 - Visualize
bike_orderlines_wrangled_location %>% 
  ggplot(aes(x=year,y=sales, fill = state)) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and category",
    subtitle = "",
    x = "Year",
    y = "Revenue",
    fill = "Main Category"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  facet_wrap(vars(state))

```


# Data Acquisition
```{r}
# WEBSCRAPING ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(dplyr)
library(tidyr)

# Challenge 1 ----
library(RedditExtractoR)
example_urls = reddit_urls(search_terms = "sport") %>%
  slice(1:10)
example_urls

# Challenge 2 ----
# 1.1 COLLECT PRODUCT FAMILIES ----

url_home          <- "https://www.rosebikes.com/bikes/mtb"

# Read in the HTML for the entire webpage
html_home         <- read_html(url_home)


# Extract the urls from the href attribute
bike_category_tbl <- html_home %>%
  
  # Going further down the tree and select nodes by class
  # Selecting two classes makes it specific enough
  html_nodes(css = ".catalog-navigation__link") %>%
  html_attr('href') %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
  
  # Add the domain, because we will get only the subdirectories
  mutate(
    url = glue("https://www.rosebikes.com{subdirectory}")
  ) %>%
  
  # Some categories are listed multiple times.
  # We only need unique values
  distinct(url)

glimpse(bike_category_tbl)

# 2.1 Get URL for each bike of the Product categories ----



get_bike_data <- function(url){
  
  html_bike_category  <- read_html(url)
  
  bike_price_tbl        <- html_bike_category %>%
    html_nodes(css = ".catalog-category-bikes__price-title") %>%
    html_text() %>%
    str_remove(pattern = "\\?.*") %>%
    enframe(name = "position", value = "price")
  
  bike_title_price_tbl        <- html_bike_category %>%
    html_nodes(css = ".catalog-category-bikes__title") %>%
    html_text() %>%
    str_remove(pattern = "\\?.*") %>%
    enframe(name = "position", value = "title") %>%
    left_join(bike_price_tbl,by = c("position" = "position"))
}
  
#bike_category_url <- bike_category_tbl$url[2]
#bike_data_tbl <- get_bike_data(bike_category_url)

#glimpse(bike_data_tbl)
# Extract the urls as a character vector
bike_category_url_vec <- bike_category_tbl %>% 
  pull(url)

# Run the function with every url as an argument
bike_data_lst <- map(bike_category_url_vec, get_bike_data)

# Merge the list into a tibble
bike_data_tbl <- bind_rows(bike_data_lst) %>%
  distinct(title, .keep_all=TRUE)
bike_data_tbl

```


# Data Wrangling
```{r}
# Tidyverse
library(tidyverse)
library(vroom)

# Data Table
library(data.table)

# Counter
library(tictoc)

library(lubridate)
# Load data ----
#Assignee
col_types <- list(
  id = col_character(),
  type = col_character(),
  name_first = col_skip(),
  name_last = col_skip(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "D:/Lucas/Uni/Data Science/DS_101/02_data_wrangling/assignee/assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#Patent_assignee
col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_skip()
)
patent_assignee_tbl <- vroom(
  file       = "D:/Lucas/Uni/Data Science/DS_101/02_data_wrangling/patent_assignee/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#Patent
col_types <- list(
  id = col_character(),
  type = col_skip(),
  number = col_character(),
  country = col_skip(),
  date = col_date("%Y-%m-%d"),
  abstract = col_skip(),
  title = col_skip(),
  kind = col_skip(),
  num_claims = col_skip(),
  filename = col_skip(),
  withdrawn = col_skip()
  
)
patent_tbl <- vroom(
  file       = "D:/Lucas/Uni/Data Science/DS_101/02_data_wrangling/patent/patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#Uspc
col_types <- list(
  uuid = col_skip(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_skip(),
  sequence = col_skip()
 
  
)
uspc_tbl <- vroom(
  file       = "D:/Lucas/Uni/Data Science/DS_101/02_data_wrangling/uspc/uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
```

## Challenges
```{r}

# Challenge 1 ----

  
first_tbl <- assignee_tbl %>% filter(type == 2) %>% 
  select(-type) %>%
  left_join(patent_assignee_tbl, by = c("id"="assignee_id")) %>% 
  group_by(organization) %>%
  count(organization,name = "count", sort=TRUE) %>%
  slice(1:10)
first_tbl


# Challenge 2 ----

assignee_patent_assignee_wrangled <- assignee_tbl %>% 
  left_join(patent_assignee_tbl, by = c("id"="assignee_id")) %>%
  select(-id)

second_tbl <- patent_tbl %>% filter (year(date) == 2019) %>%
  select (-date) %>%
  left_join(assignee_patent_assignee_wrangled, by = c("id" = "patent_id")) %>%
  filter(type == 2) %>%
  group_by(organization) %>%
  count(organization,name = "count", sort=TRUE) %>%
  slice(1:10)
second_tbl

# Challenge 3 ----
most_innovative_sector <- uspc_tbl %>%
  group_by(mainclass_id) %>%
  count(mainclass_id, name = "count", sort=TRUE) %>%
  slice(1)
most_innovative_sector


top10ww <- assignee_tbl %>% 
  select(-type) %>%
  left_join(patent_assignee_tbl, by = c("id"="assignee_id")) %>% 
  filter(!is.na(organization)) %>%
  group_by(organization) %>%
  count(organization,name = "count", sort=TRUE) %>%
  ungroup() %>%
  slice(1:10)
top10ww

top5uspto <- assignee_tbl %>% filter( organization %in% top10ww$organization) %>%
  left_join(patent_assignee_tbl, by = c("id" = "assignee_id")) %>%
  left_join(uspc_tbl, by = c("patent_id"="patent_id")) %>%
  filter(!is.na(mainclass_id)) %>%
  group_by(mainclass_id) %>%
  count(mainclass_id,name = "count", sort=TRUE) %>%
  ungroup() %>%
  slice(1:5)
top5uspto

```


# Visualisation
```{r}
# 1.0 Lollipop Chart: Top N Customers ----
library(tidyverse)
library(lubridate)

bike_orderlines_tbl <- read_rds("00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds")

n <- 10
# Data Manipulation
top_customers_tbl <- bike_orderlines_tbl %>%
  rename(bikeshop = name) %>%
  
  # Select relevant columns
  select(bikeshop, total_price) %>%
  
  # Collapse the least frequent values into "other"
  mutate(bikeshop = as_factor(bikeshop) %>% fct_lump(n = n, w = total_price)) %>%
  
  # Group and summarize
  group_by(bikeshop) %>%
  summarize(revenue = sum(total_price)) %>%
  ungroup() %>%
  
  # Reorder the column customer_city by revenue
  mutate(bikeshop = bikeshop %>% fct_reorder(revenue)) %>%
  # Place "Other" at the beginning
  mutate(bikeshop = bikeshop %>% fct_relevel("Other", after = 0)) %>%
  # Sort by this column
  arrange(desc(bikeshop)) %>%
  
  # Add Revenue Text
  mutate(revenue_text = scales::dollar(revenue, 
                                       scale  = 1e-6, 
                                       prefix = "", 
                                       suffix = "M ???")) %>%
  
  # Add Cumulative Percent
  mutate(cum_pct = cumsum(revenue) / sum(revenue)) %>%
  mutate(cum_pct_text = scales::percent(cum_pct)) %>%
  
  # Add Rank
  mutate(rank = row_number()) %>%
  mutate(rank = case_when(
    rank == max(rank) ~ NA_integer_,
    TRUE ~ rank
  )) %>%
  
  # Add Label text
  mutate(label_text = str_glue("Rank: {rank}\nRev: {revenue_text}\nCumPct: {cum_pct_text}"))

# Data Visualization
top_customers_tbl %>%
  
  # Canvas
  ggplot(aes(revenue, bikeshop)) +
  
  # Geometries
  geom_segment(aes(xend = 0, yend = bikeshop), 
               color = RColorBrewer::brewer.pal(n = 11, name = "RdBu")[11],
               size  = 1) +
  
  geom_point(aes(size = revenue),
             color = RColorBrewer::brewer.pal(n = 11, name = "RdBu")[11]) +
  
  geom_label(aes(label = label_text), 
             hjust = "inward",
             size  = 3,
             color = RColorBrewer::brewer.pal(n = 11, name = "RdBu")[11]) +
  
  # Formatting
  scale_x_continuous(labels = scales::dollar_format(scale = 1e-6, 
                                                    prefix = "",
                                                    suffix = "M ???")) +
  labs(
    title = str_glue("Top {n} Customers"),
    subtitle = str_glue(
      "Start: {year(min(bike_orderlines_tbl$order_date))}
               End:  {year(max(bike_orderlines_tbl$order_date))}"),
    x = "Revenue (M ???)",
    y = "Customer",
    caption = str_glue("Top 6 customers contribute
                           52% of purchasing power.")
  ) +
  
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold"),
    plot.caption = element_text(face = "bold.italic")
  )
```

## Challenges
```{r}

# Challenge 1 ----
library(tidyverse)
library(lubridate)
library(wesanderson)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

covid_data_filtered <- covid_data_tbl %>% 
  rename(cumulative = "Cumulative_number_for_14_days_of_COVID-19_cases_per_100000") %>%
  filter( year == 2020 & (countriesAndTerritories == "United_States_of_America"| 
                                                   countriesAndTerritories == "Spain" |
                                                   countriesAndTerritories == "Germany"|
                                                   countriesAndTerritories == "France"|
                                                   countriesAndTerritories == "United_Kingdom") )%>%
  mutate(date=as.Date(dateRep,format = "%d/%m/%Y")) %>%
  arrange(date) %>%
  group_by(countriesAndTerritories) %>%
  mutate(cum_cases = cumsum(cases))

lastdate <- covid_data_filtered %>% slice(which.max(date)) %>% filter(countriesAndTerritories == "United_States_of_America")


covid_data_filtered %>%
  ggplot(aes(x = date, y = cum_cases, color = countriesAndTerritories)) +
  geom_line(size= 1, linetype = 1) +
  scale_y_continuous(breaks = seq(0, 20e6, by = 2.5e6),labels = scales::dollar_format(scale = 1e-6, 
                                                    prefix = "",
                                                    suffix = "M",
                                                    accuracy = 0.1))+
  scale_x_date(date_label = "%B", breaks = "month")+
  geom_label(data =  lastdate, aes(label =  scales::dollar(cum_cases, 
                                        scale  = 1e-6, 
                                        prefix = "",
                                        suffix = "M")), 
            vjust = 1.5, color = "white", fill = "lightblue") +
  labs(
    title = str_glue("COVID-19 confirmed cases worldwide"),
    subtitle = str_glue(""),
    x = "Year 2020",
    y = "Cumulative Cases",
    color = "Countries / Region"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle =45),
    legend.position = "bottom",
    axis.title = element_text(face = "bold")) +
  guides(col = guide_legend(nrow = 2)) +
  scale_color_brewer(palette= "Accent")



# Challenge 2 ----
library(maps)
require(maps)
require(viridis)



covid_data_2 <- covid_data_tbl %>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
    
  )) %>% group_by(countriesAndTerritories)%>%
  summarise(mort_rate= sum(deaths)/max(popData2019))%>%
  ungroup() 

world_map <- map_data("world") %>% left_join(covid_data_2,by = c("region" = "countriesAndTerritories"))

total_deaths_ww <- sum(covid_data_tbl$deaths)*1e-5

ggplot(world_map, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = mort_rate ))+
  scale_fill_viridis_c(option = "E", end = 0.35, direction = -1,breaks = seq(0, 2e-3, by=3e-4),labels = scales::dollar_format(scale = 1e2, 
                                                                                            prefix = "",
                                                                                            suffix = "%",
                                                                                            accuracy = 0.001)) +
  labs(
    title = str_glue(" COnfirmed COVID-19 deaths relative to the size of the population"),
    subtitle = str_glue("More than {floor(total_deaths_ww)*1e-1} Million confirmed COVID-19 deaths worldwide"),
    x = "",
    y = "",
    fill = "Mortality Rate"
  ) +
  theme_minimal() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y = element_blank())

```
